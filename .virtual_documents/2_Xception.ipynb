








!git clone https://github.com/ARIM-Usecase/Example_8.git
%cd Example_8





import os
import pickle
import time
from tqdm import tqdm

#汎用ライブラリ
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2

#CNN用ライブラリ
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense
from tensorflow.keras.applications import Xception
from tensorflow.keras.optimizers import Adam





# 画像ファイルの指定
DATADIR = "data/training_data"

#分類アノテーションをフォルダ名とする
CATEGORIES = ["Biological", "Fibres", 
              "Films_Coated_Surface","MEMS_devices_and_electrodes",
              "Nanowires", "Particles", 
              "Patterned_surface", "Porous_Sponge",
              "Powder", "Tips" ]

# reshapeにおける画像サイズの指定
IMG_SIZE = 100





# データの読み込み
def load_data():
    with open("output/X.pickle", "rb") as f:
        X = pickle.load(f)
    with open("output/y.pickle", "rb") as f:
        y = pickle.load(f)
    return np.array(X) / 255.0, np.array(y)


X, y = load_data()





# 訓練データとテストデータに分割（80%:20%の比率）
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=42
                                                    )

# データ形状の確認
print(f"Training data shape: {X_train.shape}, {y_train.shape}")
print(f"Testing data shape: {X_test.shape}, {y_test.shape}")





# TensorFlowのテンソルに変換
X_train_tensor = tf.convert_to_tensor(X_train)
X_test_tensor = tf.convert_to_tensor(X_test)

# グレースケールからRGBに変換
X_train_resized = tf.image.grayscale_to_rgb(X_train_tensor)  
X_test_resized = tf.image.grayscale_to_rgb(X_test_tensor)








# 入力形状の定義
IMG_SIZE = 100
input_shape = (IMG_SIZE, IMG_SIZE, 3)  # RGB画像

model = models.Sequential()

#  Xceptionのベースモデルの構築
base_model = Xception(include_top=False, weights="imagenet", input_shape=input_shape)
model.add(base_model)  # Xceptionのベースモデルを追加
model.add(layers.GlobalAveragePooling2D())  # プーリング層を追加

# カスタム層を追加（全結合層）
# Flatten層は不要
model.add(layers.Dense(128, activation="relu"))
model.add(layers.Dense(64, activation="relu"))

# 出力層を追加（10クラス分類）
model.add(layers.Dense(10))


# モデルの概要表示
model.summary()





# モデルのコンパイル
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])





%time

# コールバック
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint('./model/best_model.keras', save_best_only=True)
]

# モデルの学習
history = model.fit(X_train_resized, y_train,
                    epochs=30,
                    batch_size=64,
                    validation_data=(X_test_resized, y_test),
                    callbacks=callbacks
                   )





train_loss,train_acc = model.evaluate(X_train_resized,y_train)
test_loss,test_acc = model.evaluate(X_test_resized,y_test)

print ('訓練データの損失関数の値', train_loss)
print ('テストデータの損失関数の値', test_loss)
print ('訓練データの分類精度',train_acc)
print ('テストデータの分類精度',test_acc)





#損失関数の値
loss = history.history['loss']
val_loss=history.history['val_loss']


#損失関数の値の可視化
plt.plot(loss, 'black', label='training')
plt.plot(val_loss, 'red', label='test')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


#分類精度
acc = history.history['accuracy']
val_acc=history.history['val_accuracy']


#分類精度の可視化

plt.plot(acc, 'black', label='training')
plt.plot(val_acc, 'red', label='test')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()





Y_pred = model.predict(X_test_resized)
Y_pred_classes = np.argmax(Y_pred, axis = 1) 

Y_true = y_test

cm = confusion_matrix(Y_true, Y_pred_classes)


import seaborn as sns

sns.heatmap(cm, 
            square=True, 
            annot=True, 
            cmap='jet', 
            fmt='.0f')

plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()





# ハイパーパラメータの設定
dense_layers = [0, 1, 2]
layer_sizes = [64, 128]
conv_layers = [0, 1]  # Xceptionの上にカスタム畳み込み層を追加するかどうか

# 精度記録用変数
best_accuracy = 0
best_params = {}
best_model = None
best_model_name = ""





for dense_layer in dense_layers:
    for layer_size in layer_sizes:
        for conv_layer in conv_layers:
            NAME = f"{conv_layer}-conv-{layer_size}-nodes-{dense_layer}-dense-{int(time.time())}"
            print(NAME)

            # Sequentialモデルの構築
            model = models.Sequential()

            # Xceptionベースモデルの追加
            base_model = Xception(include_top=False, weights="imagenet", input_shape=(100, 100, 3))
            base_model.trainable = False  
            model.add(base_model)

            # Optional: カスタム畳み込み層の追加
            for _ in range(conv_layer):
                model.add(layers.Conv2D(layer_size, (3, 3), activation='relu'))

            # GlobalAveragePooling2Dを追加
            model.add(layers.GlobalAveragePooling2D())

            # 全結合層の追加
            for _ in range(dense_layer):
                model.add(layers.Dense(layer_size, activation='relu'))

            # 出力層
            model.add(layers.Dense(10, activation='softmax'))

            # モデルのコンパイル
            model.compile(optimizer=Adam(),
                          loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                          metrics=['accuracy'])

            # モデルの学習
            history = model.fit(X_train_resized, y_train,
                                epochs=10,
                                batch_size=64,
                                validation_data=(X_test_resized, y_test),
                                verbose=1)

            # 検証精度を取得
            val_accuracy = max(history.history['val_accuracy'])
            print(f"Validation accuracy for {NAME}: {val_accuracy}")

            # 精度が高いモデルを記録
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                
                best_params = {
                    "conv_layers": conv_layer,
                    "layer_size": layer_size,
                    "dense_layers": dense_layer,
                    "model_name": NAME
                }
                
                best_model = model  # 最良モデルを保存
                best_model_name = NAME  # 最良モデルの名前を保存

# 最適なハイパーパラメータと精度を出力
print(f"Best validation accuracy: {best_accuracy}")
print(f"Best model parameters: {best_params}")





if best_model is not None:
    best_model.save(f'model/{best_model_name}.keras') 
    print(f"Best model saved as 'model/{best_model_name}.keras'")





model_directory='model'


def load_best_model(model_directory=model_directory):
    model_files = [f for f in os.listdir(model_directory) if f.endswith('.keras')]
    if not model_files:
        raise FileNotFoundError("No model files found in the specified directory.")
    
    # 最も新しいファイルを選択
    best_model_file = max(model_files, key=lambda x: os.path.getmtime(os.path.join(model_directory, x)))
    return tf.keras.models.load_model(os.path.join(model_directory, best_model_file))



# モデルの読み込み
model = load_best_model()





%time

# コールバック
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint('./model/best_model.keras', save_best_only=True)
]

# モデルの学習
history = model.fit(X_train_resized, y_train,
                    epochs=30,
                    batch_size=64,
                    validation_data=(X_test_resized, y_test),
                    callbacks=callbacks)


# 損失関数の値を取得
loss = history.history['loss']
val_loss = history.history['val_loss']


#損失関数の値の可視化
plt.plot(loss, 'black', label='training')
plt.plot(val_loss, 'red', label='test')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


#分類精度
acc = history.history['accuracy']
val_acc=history.history['val_accuracy']


#分類精度の可視化
plt.plot(acc, 'black', label='training')
plt.plot(val_acc, 'red', label='test')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


Y_pred = model.predict(X_test_resized)
Y_pred_classes = np.argmax(Y_pred, axis = 1) 

Y_true = y_test

cm = confusion_matrix(Y_true, Y_pred_classes)

sns.heatmap(cm, 
            square=True, 
            annot=True, 
            cmap='jet', 
            fmt='.0f')

plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()





# 画像の前処理を行う関数 
def prepare(filepath, img_size=100):
    img_array = cv2.imread(filepath, cv2.IMREAD_COLOR)  # RGB形式で読み込む
    img_array = cv2.resize(img_array, (img_size, img_size))  # 画像サイズをリサイズ
    return img_array.reshape(-1, img_size, img_size, 3)  # 形状を調整


# Xceptionモデルの構築
base_model = Xception(include_top=False, weights="imagenet", input_shape=(100, 100, 3))  # 事前学習済みモデルを使用
base_model.trainable = False  # 事前学習済みモデルをフリーズ

# 出力層を追加
model = models.Sequential()

model.add(base_model)  
model.add(layers.GlobalAveragePooling2D()) 
model.add(layers.Dense(len(CATEGORIES), activation='softmax')) 


# 予測を行う関数 
@tf.function  # TensorFlow関数としてデコレート
def predict_image(image_array):
    prediction = model(image_array) 
    return prediction


# 未知の画像に対する予測
image_paths = ['data/prediction_data/biology.jpg', 'data/prediction_data/fiber.jpg']

for img_path in image_paths:
    prepared_image = prepare(img_path)
    prediction = predict_image(prepared_image)

    print(f"Prediction shape: {prediction.shape}")  # 形状を表示して確認

    predicted_index = np.argmax(prediction[0])  # 最大の確率のインデックスを取得
    if predicted_index < len(CATEGORIES):  # インデックスがリストの範囲内か確認
        predicted_class = CATEGORIES[predicted_index] 
    else:
        predicted_class = "Unknown"  # 範囲外の場合はUnknownとする
        
    print(f"Image: {os.path.basename(img_path)}")
    print(f"Predicted category: {predicted_class}")
    print(f"Prediction probabilities: {prediction}\n")
