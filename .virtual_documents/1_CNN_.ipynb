








!git clone https://github.com/ARIM-Usecase/Example_8.git
%cd Example_8





import os
import pickle
import time
from tqdm import tqdm

#汎用ライブラリ
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2

#CNN用ライブラリ
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow.keras import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense





# 画像ファイルの指定
DATADIR = "data/training_data"

#分類アノテーションをフォルダ名とする
CATEGORIES = ["Biological", "Fibres", 
              "Films_Coated_Surface","MEMS_devices_and_electrodes",
              "Nanowires", "Particles", 
              "Patterned_surface", "Porous_Sponge",
              "Powder", "Tips" ]

# reshapeにおける画像サイズの指定
IMG_SIZE = 100





#画像ファイルのチェック
for category in CATEGORIES: 
    path = os.path.join(DATADIR,category)  
    
    for img in os.listdir(path): 
        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE) 
        plt.imshow(img_array, cmap='gray') 
        plt.show()  

        break  
    break


#画像サイズ
print(img_array.shape) 





def create_training_data(data_dir, categories, img_size):
    training_data = []
    
    for category in categories:
        path = os.path.join(data_dir, category)
        class_num = categories.index(category)
        
        for img in tqdm(os.listdir(path)):
            try:
                img_path = os.path.join(path, img)
                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                
                if img_array is None:
                    raise ValueError(f"Unable to read image at {img_path}")
                
                resized_img = cv2.resize(img_array, (img_size, img_size))
                training_data.append([resized_img, class_num])
            
            except Exception as e:
                print(f"Error processing image {img}: {e}")
    
    return training_data


training_data = create_training_data(DATADIR, CATEGORIES, IMG_SIZE)





def prepare_data(training_data, img_size):
    X = np.array([features for features, _ in training_data]).reshape(-1, img_size, img_size, 1)
    y = np.array([label for _, label in training_data])
    
    return X, y



X, y = prepare_data(training_data, IMG_SIZE)





def save_pickle(data, filename):
    with open(filename, "wb") as file:
        pickle.dump(data, file)


save_pickle(X, "output/X.pickle")
save_pickle(y, "output/y.pickle")





# データの読み込み
def load_data():
    with open("output/X.pickle", "rb") as f:
        X = pickle.load(f)
    with open("output/y.pickle", "rb") as f:
        y = pickle.load(f)
    return np.array(X) / 255.0, np.array(y)


X, y = load_data()





# 訓練データとテストデータに分割（80%:20%の比率）
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=42
                                                    )

# データ形状の確認
print(f"Training data shape: {X_train.shape}, {y_train.shape}")
print(f"Testing data shape: {X_test.shape}, {y_test.shape}")





input_shape = (IMG_SIZE, IMG_SIZE, 1)  # グレースケール画像の場合

model = Sequential()

# 入力層を最初に追加
model.add(Input(shape=input_shape))

# 畳み込み層を追加
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))


# 平坦化層と全結合層
model.add(Flatten())
model.add(Dense(units=128, activation="relu"))
model.add(Dense(units=64, activation="relu"))
model.add(Dense(units=10)) # 出力層（10クラス分類）


# モデルの概要表示
model.summary()





# モデルのコンパイル
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])






%time
# モデルの学習
history = model.fit(X_train, y_train, 
                    epochs=50, 
                    batch_size=64,
                    validation_data=(X_test,y_test)
                    )





train_loss,train_acc = model.evaluate(X_train,y_train)
test_loss,test_acc = model.evaluate(X_test,y_test)

print ('訓練データの損失関数の値', train_loss)
print ('テストデータの損失関数の値', test_loss)
print ('訓練データの分類精度',train_acc)
print ('テストデータの分類精度',test_acc)





#損失関数の値
loss = history.history['loss']
val_loss=history.history['val_loss']


#損失関数の値の可視化
plt.plot(loss, 'black', label='training')
plt.plot(val_loss, 'red', label='test')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


#分類精度
acc = history.history['accuracy']
val_acc=history.history['val_accuracy']


#分類精度の可視化

plt.plot(acc, 'black', label='training')
plt.plot(val_acc, 'red', label='test')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()





Y_pred = model.predict(X_test)
Y_pred_classes = np.argmax(Y_pred, axis = 1) 

Y_true = y_test

cm = confusion_matrix(Y_true, Y_pred_classes)


import seaborn as sns

sns.heatmap(cm, 
            square=True, 
            annot=True, 
            cmap='jet', 
            fmt='.0f')

plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()





# ハイパーパラメータの設定
dense_layers = [0]
layer_sizes = [64]
conv_layers = [1, 2, 3]

# 精度記録用変数
best_accuracy = 0
best_params = {}
best_model = None
best_model_name = ""





# モデル最適化のためのループ
for dense_layer in dense_layers:
    for layer_size in layer_sizes:
        for conv_layer in conv_layers:
            NAME = f"{conv_layer}-conv-{layer_size}-nodes-{dense_layer}-{int(time.time())}"
            print(NAME)

            model = Sequential()
            model.add(Input(shape=X.shape[1:]))  # Input層を追加

            # 最初の畳み込み層
            model.add(Conv2D(layer_size, (3, 3)))
            model.add(Activation('relu'))
            model.add(MaxPooling2D(pool_size=(2, 2)))

            # 追加の畳み込み層
            for _ in range(conv_layer - 1):
                model.add(Conv2D(layer_size, (3, 3)))
                model.add(Activation('relu'))
                model.add(MaxPooling2D(pool_size=(2, 2)))

            # 平坦化と全結合層
            model.add(Flatten())
            for _ in range(dense_layer):
                model.add(Dense(layer_size))
                model.add(Activation('relu'))

            # 出力層
            model.add(Dense(10, activation='softmax'))

            # モデルのコンパイル
            model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                          optimizer='adam',
                          metrics=['accuracy'])

            # モデルの学習
            history = model.fit(X, y,
                                batch_size=64,
                                epochs=10,
                                validation_split=0.2,
                                verbose=1)

            # 検証精度を取得
            val_accuracy = max(history.history['val_accuracy'])
            print(f"Validation accuracy for {NAME}: {val_accuracy}")

            # 精度が高いモデルを記録
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                
                best_params = {
                    "conv_layers": conv_layer,
                    "layer_size": layer_size,
                    "dense_layers": dense_layer,
                    "model_name": NAME
                }
                
                best_model = model  # 最良モデルを保存
                best_model_name = NAME  # 最良モデルの名前を保存

# 最適なハイパーパラメータと精度を出力
print(f"Best validation accuracy: {best_accuracy}")
print(f"Best model parameters: {best_params}")





if best_model is not None:
    best_model.save(f'model/{best_model_name}.keras') 
    print(f"Best model saved as 'model/{best_model_name}.keras'")





model_directory='model'


def load_best_model(model_directory=model_directory):
    model_files = [f for f in os.listdir(model_directory) if f.endswith('.keras')]
    if not model_files:
        raise FileNotFoundError("No model files found in the specified directory.")
    
    # 最も新しいファイルを選択
    best_model_file = max(model_files, key=lambda x: os.path.getmtime(os.path.join(model_directory, x)))
    return tf.keras.models.load_model(os.path.join(model_directory, best_model_file))



# モデルの読み込み
model = load_best_model()





# モデルの再トレーニング（過去の履歴がある場合は省略可能）
history = model.fit(X, y,
                    batch_size=64,
                    epochs=50,
                    validation_split=0.2
                   )


# 損失関数の値を取得
loss = history.history['loss']
val_loss = history.history['val_loss']


#損失関数の値の可視化
plt.plot(loss, 'black', label='training')
plt.plot(val_loss, 'red', label='test')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


#分類精度
acc = history.history['accuracy']
val_acc=history.history['val_accuracy']


#分類精度の可視化
plt.plot(acc, 'black', label='training')
plt.plot(val_acc, 'red', label='test')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


Y_pred = model.predict(X_test)
Y_pred_classes = np.argmax(Y_pred, axis = 1) 

Y_true = y_test

cm = confusion_matrix(Y_true, Y_pred_classes)

sns.heatmap(cm, 
            square=True, 
            annot=True, 
            cmap='jet', 
            fmt='.0f')

plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()





# 予測を行う関数
def predict_image(filepath):
    prediction = model.predict(prepare(filepath))
    predicted_class = CATEGORIES[int(np.argmax(prediction[0]))]  # 最大の確率のインデックスを取得しカテゴリ名を取得
    return predicted_class, prediction



# 画像の前処理を行う関数
def prepare(filepath, img_size=100):
    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)  # グレースケールで読み込む
    img_array = cv2.resize(img_array, (img_size, img_size))  # 画像サイズをリサイズ
    return img_array.reshape(-1, img_size, img_size, 1)  # 形状を調整


# 未知の画像に対する予測
image_paths = ['data/prediction_data/biology.jpg', 'data/prediction_data/fiber.jpg']

for img_path in image_paths:
    category, pred = predict_image(img_path)
    print(f"Image: {os.path.basename(img_path)}")
    print(f"Predicted category: {category}")
    print(f"Prediction probabilities: {pred}\n")
